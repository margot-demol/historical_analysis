Warning: no access to tty (Bad file descriptor).
Thus no job control in this shell.
Fri Nov 10 18:00:16 GMT 2023
/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/dask/config.py:742: FutureWarning: Dask configuration key 'distributed.scheduler.transition-log-length' has been deprecated; please use 'distributed.admin.low-level-log-length' instead
  warnings.warn(
/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/dask/config.py:742: FutureWarning: Dask configuration key 'distributed.comm.recent-messages-log-length' has been deprecated; please use 'distributed.admin.low-level-log-length' instead
  warnings.warn(
tornado.application - ERROR - Exception in callback <bound method BokehTornado._keep_alive of <bokeh.server.tornado.BokehTornado object at 0x2aaad0a3ddc0>>
Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/tornado/ioloop.py", line 919, in _run
    val = self.callback()
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/bokeh/server/tornado.py", line 779, in _keep_alive
    c.send_ping()
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/bokeh/server/connection.py", line 91, in send_ping
    self._socket.ping(str(self._ping_count).encode("utf-8"))
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/tornado/websocket.py", line 439, in ping
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
tornado.application - ERROR - Exception in callback <bound method ActiveMemoryManagerExtension.run_once of <distributed.active_memory_manager.ActiveMemoryManagerExtension object at 0x2aaad0ad4250>>
Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/tornado/ioloop.py", line 919, in _run
    val = self.callback()
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/distributed/utils.py", line 838, in wrapper
    return func(*args, **kwargs)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/distributed/active_memory_manager.py", line 178, in run_once
    self._enact_suggestions()
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/distributed/active_memory_manager.py", line 423, in _enact_suggestions
    self.scheduler.request_remove_replicas(
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/distributed/scheduler.py", line 8229, in request_remove_replicas
    self.stream_comms[addr].send(
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/distributed/batched.py", line 156, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Scheduler connection to worker local=tcp://10.148.0.131:57336 remote=tcp://10.148.0.93:37364> already closed.
Number of workers up = 0
Number of workers up = 0
Number of workers up = 0
Number of workers up = 0
Number of workers up = 2
Number of workers up = 60
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 135
Number of workers up = 150
Number of workers up = 150
Number of workers up = 195
Cluster is up, proceeding with computations
Traceback (most recent call last):
  File "/home1/datahome/mdemol/historical_analysis/job_distributed.py", line 306, in <module>
    if dataset == 'aviso' : run_aviso(l, cluster, client)
  File "/home1/datahome/mdemol/historical_analysis/job_distributed.py", line 247, in run_aviso
    ds_aviso.to_zarr(zarr, mode="w")
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/core/dataset.py", line 2036, in to_zarr
    return to_zarr(
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/api.py", line 1431, in to_zarr
    dump_to_store(dataset, zstore, writer, encoding=encoding)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/api.py", line 1119, in dump_to_store
    store.store(variables, attrs, check_encoding, writer, unlimited_dims=unlimited_dims)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/zarr.py", line 500, in store
    variables_encoded, attributes = self.encode(
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/common.py", line 200, in encode
    variables = {k: self.encode_variable(v) for k, v in variables.items()}
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/common.py", line 200, in <dictcomp>
    variables = {k: self.encode_variable(v) for k, v in variables.items()}
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/zarr.py", line 459, in encode_variable
    variable = encode_zarr_variable(variable)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/zarr.py", line 258, in encode_zarr_variable
    var = conventions.encode_cf_variable(var, name=name)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/conventions.py", line 273, in encode_cf_variable
    var = coder.encode(var, name=name)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/coding/times.py", line 659, in encode
    (data, units, calendar) = encode_cf_datetime(
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/coding/times.py", line 592, in encode_cf_datetime
    dates = np.asarray(dates)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/dask/array/core.py", line 1700, in __array__
    x = self.compute()
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/dask/base.py", line 342, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/dask/base.py", line 628, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/distributed/client.py", line 2244, in _gather
    raise exception.with_traceback(traceback)
distributed.scheduler.KilledWorker: Attempted to run task ('open_dataset-getitem-af01040a0b01fe89eef94900b48cfe99', 12, 7) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://10.148.0.94:38683. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.
Sat Nov 11 00:08:29 GMT 2023

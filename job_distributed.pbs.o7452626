Warning: no access to tty (Bad file descriptor).
Thus no job control in this shell.
Thu Nov 16 22:11:21 GMT 2023
/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/dask/config.py:742: FutureWarning: Dask configuration key 'distributed.scheduler.transition-log-length' has been deprecated; please use 'distributed.admin.low-level-log-length' instead
  warnings.warn(
/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/dask/config.py:742: FutureWarning: Dask configuration key 'distributed.comm.recent-messages-log-length' has been deprecated; please use 'distributed.admin.low-level-log-length' instead
  warnings.warn(
Number of workers up = 0
Number of workers up = 0
Number of workers up = 0
Number of workers up = 0
Number of workers up = 0
Number of workers up = 0
Number of workers up = 20
Number of workers up = 40
Number of workers up = 200
Cluster is up, proceeding with computations
Traceback (most recent call last):
  File "/home1/datahome/mdemol/historical_analysis/job_distributed.py", line 307, in <module>
    if dataset == 'eras' : run_eras(l, cluster, client)
  File "/home1/datahome/mdemol/historical_analysis/job_distributed.py", line 262, in run_eras
    ds_es.to_zarr(zarr, mode="w")  
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/core/dataset.py", line 2036, in to_zarr
    return to_zarr(
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/api.py", line 1431, in to_zarr
    dump_to_store(dataset, zstore, writer, encoding=encoding)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/api.py", line 1119, in dump_to_store
    store.store(variables, attrs, check_encoding, writer, unlimited_dims=unlimited_dims)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/zarr.py", line 500, in store
    variables_encoded, attributes = self.encode(
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/common.py", line 200, in encode
    variables = {k: self.encode_variable(v) for k, v in variables.items()}
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/common.py", line 200, in <dictcomp>
    variables = {k: self.encode_variable(v) for k, v in variables.items()}
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/zarr.py", line 459, in encode_variable
    variable = encode_zarr_variable(variable)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/zarr.py", line 258, in encode_zarr_variable
    var = conventions.encode_cf_variable(var, name=name)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/conventions.py", line 273, in encode_cf_variable
    var = coder.encode(var, name=name)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/coding/times.py", line 659, in encode
    (data, units, calendar) = encode_cf_datetime(
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/coding/times.py", line 592, in encode_cf_datetime
    dates = np.asarray(dates)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/dask/array/core.py", line 1700, in __array__
    x = self.compute()
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/dask/base.py", line 342, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/dask/base.py", line 628, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/distributed/client.py", line 2244, in _gather
    raise exception.with_traceback(traceback)
distributed.scheduler.KilledWorker: Attempted to run task 'original-open_dataset-ce1382a1c315329fd7571eba2db83959alti_ggx_sla_unfiltered_imf1-970637e10fae73ac6e6fec18573d68e2' on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://10.148.1.62:52689. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.
Fri Nov 17 02:20:21 GMT 2023

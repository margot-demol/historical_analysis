Warning: no access to tty (Bad file descriptor).
Thus no job control in this shell.
Mon Nov 13 15:59:29 GMT 2023
/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/dask/config.py:742: FutureWarning: Dask configuration key 'distributed.scheduler.transition-log-length' has been deprecated; please use 'distributed.admin.low-level-log-length' instead
  warnings.warn(
/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/dask/config.py:742: FutureWarning: Dask configuration key 'distributed.comm.recent-messages-log-length' has been deprecated; please use 'distributed.admin.low-level-log-length' instead
  warnings.warn(
tornado.application - ERROR - Uncaught exception GET /status/ws (10.148.1.145)
HTTPServerRequest(protocol='http', host='localhost:8787', method='GET', uri='/status/ws', version='HTTP/1.1', remote_ip='10.148.1.145')
Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/tornado/websocket.py", line 937, in _accept_connection
    open_result = handler.open(*handler.open_args, **handler.open_kwargs)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/tornado/web.py", line 3290, in wrapper
    return method(self, *args, **kwargs)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/bokeh/server/views/ws.py", line 149, in open
    raise ProtocolError("Token is expired.")
bokeh.protocol.exceptions.ProtocolError: Token is expired.
Number of workers up = 0
Number of workers up = 0
Number of workers up = 0
Number of workers up = 0
Number of workers up = 0
Number of workers up = 0
Number of workers up = 0
Number of workers up = 100
Number of workers up = 100
Number of workers up = 120
Number of workers up = 200
Cluster is up, proceeding with computations
Traceback (most recent call last):
  File "/home1/datahome/mdemol/historical_analysis/job_distributed.py", line 307, in <module>
    if dataset == 'eras' : run_eras(l, cluster, client)
  File "/home1/datahome/mdemol/historical_analysis/job_distributed.py", line 262, in run_eras
    ds_es.chunk({'obs':500}).to_zarr(zarr, mode="w")  
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/core/dataset.py", line 2036, in to_zarr
    return to_zarr(
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/api.py", line 1431, in to_zarr
    dump_to_store(dataset, zstore, writer, encoding=encoding)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/api.py", line 1119, in dump_to_store
    store.store(variables, attrs, check_encoding, writer, unlimited_dims=unlimited_dims)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/zarr.py", line 500, in store
    variables_encoded, attributes = self.encode(
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/common.py", line 200, in encode
    variables = {k: self.encode_variable(v) for k, v in variables.items()}
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/common.py", line 200, in <dictcomp>
    variables = {k: self.encode_variable(v) for k, v in variables.items()}
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/zarr.py", line 459, in encode_variable
    variable = encode_zarr_variable(variable)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/backends/zarr.py", line 258, in encode_zarr_variable
    var = conventions.encode_cf_variable(var, name=name)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/conventions.py", line 273, in encode_cf_variable
    var = coder.encode(var, name=name)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/coding/times.py", line 659, in encode
    (data, units, calendar) = encode_cf_datetime(
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/xarray/coding/times.py", line 592, in encode_cf_datetime
    dates = np.asarray(dates)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/dask/array/core.py", line 1700, in __array__
    x = self.compute()
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/dask/base.py", line 342, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/dask/base.py", line 628, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/home1/datahome/mdemol/.miniconda3/envs/histenv2/lib/python3.9/site-packages/distributed/client.py", line 2244, in _gather
    raise exception.with_traceback(traceback)
distributed.scheduler.KilledWorker: Attempted to run task ('box_y-_concat_eras-0e4efb988b498e508bb59256046f3b28-e694257cad1b0ad40838cf63ec8abe28', 83, 0, 0, 0, 0, 0) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://10.148.0.237:57867. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.
Mon Nov 13 22:04:26 GMT 2023
